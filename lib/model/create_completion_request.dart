//
// AUTO-GENERATED FILE, DO NOT MODIFY!
//
// @dart=2.18

// ignore_for_file: unused_element, unused_import
// ignore_for_file: always_put_required_named_parameters_first
// ignore_for_file: constant_identifier_names
// ignore_for_file: lines_longer_than_80_chars

part of openapi.api;

class CreateCompletionRequest {
  /// Returns a new [CreateCompletionRequest] instance.
  CreateCompletionRequest({
    required this.model,
    required this.prompt,
    this.bestOf = 1,
    this.echo = false,
    this.frequencyPenalty = 0,
    this.logitBias = const {},
    this.logprobs,
    this.maxTokens = 16,
    this.n = 1,
    this.presencePenalty = 0,
    this.seed,
    this.stop,
    this.stream = false,
    this.streamOptions,
    this.suffix,
    this.temperature = 1,
    this.topP = 1,
    this.user,
  });

  CreateCompletionRequestModel model;

  CreateCompletionRequestPrompt prompt;

  /// Generates `best_of` completions server-side and returns the \"best\" (the one with the highest log probability per token). Results cannot be streamed.  When used with `n`, `best_of` controls the number of candidate completions and `n` specifies how many to return â€“ `best_of` must be greater than `n`.  **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`. 
  ///
  /// Minimum value: 0
  /// Maximum value: 20
  int bestOf;

  /// Echo back the prompt in addition to the completion 
  bool echo;

  /// Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/guides/text-generation) 
  ///
  /// Minimum value: -2
  /// Maximum value: 2
  num frequencyPenalty;

  /// Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this [tokenizer tool](/tokenizer?view=bpe) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.  As an example, you can pass `{\"50256\": -100}` to prevent the <|endoftext|> token from being generated. 
  Map<String, int> logitBias;

  /// Include the log probabilities on the `logprobs` most likely output tokens, as well the chosen tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1` elements in the response.  The maximum value for `logprobs` is 5. 
  ///
  /// Minimum value: 0
  /// Maximum value: 5
  ///
  /// Please note: This property should have been non-nullable! Since the specification file
  /// does not include a default value (using the "default:" property), however, the generated
  /// source code must fall back to having a nullable type.
  /// Consider adding a "default:" property in the specification file to hide this note.
  ///
  int? logprobs;

  /// The maximum number of [tokens](/tokenizer) that can be generated in the completion.  The token count of your prompt plus `max_tokens` cannot exceed the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens. 
  ///
  /// Minimum value: 0
  int maxTokens;

  /// How many completions to generate for each prompt.  **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`. 
  ///
  /// Minimum value: 1
  /// Maximum value: 128
  int n;

  /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/guides/text-generation) 
  ///
  /// Minimum value: -2
  /// Maximum value: 2
  num presencePenalty;

  /// If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.  Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend. 
  ///
  /// Please note: This property should have been non-nullable! Since the specification file
  /// does not include a default value (using the "default:" property), however, the generated
  /// source code must fall back to having a nullable type.
  /// Consider adding a "default:" property in the specification file to hide this note.
  ///
  int? seed;

  ///
  /// Please note: This property should have been non-nullable! Since the specification file
  /// does not include a default value (using the "default:" property), however, the generated
  /// source code must fall back to having a nullable type.
  /// Consider adding a "default:" property in the specification file to hide this note.
  ///
  StopConfiguration? stop;

  /// Whether to stream back partial progress. If set, tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions). 
  bool stream;

  ///
  /// Please note: This property should have been non-nullable! Since the specification file
  /// does not include a default value (using the "default:" property), however, the generated
  /// source code must fall back to having a nullable type.
  /// Consider adding a "default:" property in the specification file to hide this note.
  ///
  ChatCompletionStreamOptions? streamOptions;

  /// The suffix that comes after a completion of inserted text.  This parameter is only supported for `gpt-3.5-turbo-instruct`. 
  ///
  /// Please note: This property should have been non-nullable! Since the specification file
  /// does not include a default value (using the "default:" property), however, the generated
  /// source code must fall back to having a nullable type.
  /// Consider adding a "default:" property in the specification file to hide this note.
  ///
  String? suffix;

  /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or `top_p` but not both. 
  ///
  /// Minimum value: 0
  /// Maximum value: 2
  num temperature;

  /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or `temperature` but not both. 
  ///
  /// Minimum value: 0
  /// Maximum value: 1
  num topP;

  /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids). 
  ///
  /// Please note: This property should have been non-nullable! Since the specification file
  /// does not include a default value (using the "default:" property), however, the generated
  /// source code must fall back to having a nullable type.
  /// Consider adding a "default:" property in the specification file to hide this note.
  ///
  String? user;

  @override
  bool operator ==(Object other) => identical(this, other) || other is CreateCompletionRequest &&
    other.model == model &&
    other.prompt == prompt &&
    other.bestOf == bestOf &&
    other.echo == echo &&
    other.frequencyPenalty == frequencyPenalty &&
    _deepEquality.equals(other.logitBias, logitBias) &&
    other.logprobs == logprobs &&
    other.maxTokens == maxTokens &&
    other.n == n &&
    other.presencePenalty == presencePenalty &&
    other.seed == seed &&
    other.stop == stop &&
    other.stream == stream &&
    other.streamOptions == streamOptions &&
    other.suffix == suffix &&
    other.temperature == temperature &&
    other.topP == topP &&
    other.user == user;

  @override
  int get hashCode =>
    // ignore: unnecessary_parenthesis
    (model.hashCode) +
    (prompt.hashCode) +
    (bestOf.hashCode) +
    (echo.hashCode) +
    (frequencyPenalty.hashCode) +
    (logitBias.hashCode) +
    (logprobs == null ? 0 : logprobs!.hashCode) +
    (maxTokens.hashCode) +
    (n.hashCode) +
    (presencePenalty.hashCode) +
    (seed == null ? 0 : seed!.hashCode) +
    (stop == null ? 0 : stop!.hashCode) +
    (stream.hashCode) +
    (streamOptions == null ? 0 : streamOptions!.hashCode) +
    (suffix == null ? 0 : suffix!.hashCode) +
    (temperature.hashCode) +
    (topP.hashCode) +
    (user == null ? 0 : user!.hashCode);

  @override
  String toString() => 'CreateCompletionRequest[model=$model, prompt=$prompt, bestOf=$bestOf, echo=$echo, frequencyPenalty=$frequencyPenalty, logitBias=$logitBias, logprobs=$logprobs, maxTokens=$maxTokens, n=$n, presencePenalty=$presencePenalty, seed=$seed, stop=$stop, stream=$stream, streamOptions=$streamOptions, suffix=$suffix, temperature=$temperature, topP=$topP, user=$user]';

  Map<String, dynamic> toJson() {
    final json = <String, dynamic>{};
      json[r'model'] = this.model;
      json[r'prompt'] = this.prompt;
      json[r'best_of'] = this.bestOf;
      json[r'echo'] = this.echo;
      json[r'frequency_penalty'] = this.frequencyPenalty;
      json[r'logit_bias'] = this.logitBias;
    if (this.logprobs != null) {
      json[r'logprobs'] = this.logprobs;
    } else {
      json[r'logprobs'] = null;
    }
      json[r'max_tokens'] = this.maxTokens;
      json[r'n'] = this.n;
      json[r'presence_penalty'] = this.presencePenalty;
    if (this.seed != null) {
      json[r'seed'] = this.seed;
    } else {
      json[r'seed'] = null;
    }
    if (this.stop != null) {
      json[r'stop'] = this.stop;
    } else {
      json[r'stop'] = null;
    }
      json[r'stream'] = this.stream;
    if (this.streamOptions != null) {
      json[r'stream_options'] = this.streamOptions;
    } else {
      json[r'stream_options'] = null;
    }
    if (this.suffix != null) {
      json[r'suffix'] = this.suffix;
    } else {
      json[r'suffix'] = null;
    }
      json[r'temperature'] = this.temperature;
      json[r'top_p'] = this.topP;
    if (this.user != null) {
      json[r'user'] = this.user;
    } else {
      json[r'user'] = null;
    }
    return json;
  }

  /// Returns a new [CreateCompletionRequest] instance and imports its values from
  /// [value] if it's a [Map], null otherwise.
  // ignore: prefer_constructors_over_static_methods
  static CreateCompletionRequest? fromJson(dynamic value) {
    if (value is Map) {
      final json = value.cast<String, dynamic>();

      // Ensure that the map contains the required keys.
      // Note 1: the values aren't checked for validity beyond being non-null.
      // Note 2: this code is stripped in release mode!
      assert(() {
        requiredKeys.forEach((key) {
          assert(json.containsKey(key), 'Required key "CreateCompletionRequest[$key]" is missing from JSON.');
          assert(json[key] != null, 'Required key "CreateCompletionRequest[$key]" has a null value in JSON.');
        });
        return true;
      }());

      return CreateCompletionRequest(
        model: CreateCompletionRequestModel.fromJson(json[r'model'])!,
        prompt: CreateCompletionRequestPrompt.fromJson(json[r'prompt'])!,
        bestOf: mapValueOfType<int>(json, r'best_of') ?? 1,
        echo: mapValueOfType<bool>(json, r'echo') ?? false,
        frequencyPenalty: num.parse('${json[r'frequency_penalty']}'),
        logitBias: mapCastOfType<String, int>(json, r'logit_bias') ?? const {},
        logprobs: mapValueOfType<int>(json, r'logprobs'),
        maxTokens: mapValueOfType<int>(json, r'max_tokens') ?? 16,
        n: mapValueOfType<int>(json, r'n') ?? 1,
        presencePenalty: num.parse('${json[r'presence_penalty']}'),
        seed: mapValueOfType<int>(json, r'seed'),
        stop: StopConfiguration.fromJson(json[r'stop']),
        stream: mapValueOfType<bool>(json, r'stream') ?? false,
        streamOptions: ChatCompletionStreamOptions.fromJson(json[r'stream_options']),
        suffix: mapValueOfType<String>(json, r'suffix'),
        temperature: num.parse('${json[r'temperature']}'),
        topP: num.parse('${json[r'top_p']}'),
        user: mapValueOfType<String>(json, r'user'),
      );
    }
    return null;
  }

  static List<CreateCompletionRequest> listFromJson(dynamic json, {bool growable = false,}) {
    final result = <CreateCompletionRequest>[];
    if (json is List && json.isNotEmpty) {
      for (final row in json) {
        final value = CreateCompletionRequest.fromJson(row);
        if (value != null) {
          result.add(value);
        }
      }
    }
    return result.toList(growable: growable);
  }

  static Map<String, CreateCompletionRequest> mapFromJson(dynamic json) {
    final map = <String, CreateCompletionRequest>{};
    if (json is Map && json.isNotEmpty) {
      json = json.cast<String, dynamic>(); // ignore: parameter_assignments
      for (final entry in json.entries) {
        final value = CreateCompletionRequest.fromJson(entry.value);
        if (value != null) {
          map[entry.key] = value;
        }
      }
    }
    return map;
  }

  // maps a json object with a list of CreateCompletionRequest-objects as value to a dart map
  static Map<String, List<CreateCompletionRequest>> mapListFromJson(dynamic json, {bool growable = false,}) {
    final map = <String, List<CreateCompletionRequest>>{};
    if (json is Map && json.isNotEmpty) {
      // ignore: parameter_assignments
      json = json.cast<String, dynamic>();
      for (final entry in json.entries) {
        map[entry.key] = CreateCompletionRequest.listFromJson(entry.value, growable: growable,);
      }
    }
    return map;
  }

  /// The list of required keys that must be present in a JSON.
  static const requiredKeys = <String>{
    'model',
    'prompt',
  };
}

